<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.353">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Hyperparameter selection</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="Hyperparameter_selection_files/libs/clipboard/clipboard.min.js"></script>
<script src="Hyperparameter_selection_files/libs/quarto-html/quarto.js"></script>
<script src="Hyperparameter_selection_files/libs/quarto-html/popper.min.js"></script>
<script src="Hyperparameter_selection_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="Hyperparameter_selection_files/libs/quarto-html/anchor.min.js"></script>
<link href="Hyperparameter_selection_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="Hyperparameter_selection_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="Hyperparameter_selection_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="Hyperparameter_selection_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="Hyperparameter_selection_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>


</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Hyperparameter selection</h1>
</div>



<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">Invalid Date</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<p>We will work on a dataset containing concrete characteristics, and try to predict an output variable. This is a regression problem, and we will use four different algorithms: bagging, random forests, adaboost and gradient boosting. This project should help us get an introduction to the use of these algorithms, as well as exploring some strategies for selecting hyperparameters.</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy <span class="im">import</span> mean</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>warnings.filterwarnings(<span class="st">"ignore"</span>, category<span class="op">=</span><span class="pp">DeprecationWarning</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> GradientBoostingClassifier</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> KFold</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> make_scorer, mean_squared_error</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>plt.style.use(<span class="st">'bmh'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="data-preparation" class="level2">
<h2 class="anchored" data-anchor-id="data-preparation">Data preparation</h2>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># requires xlrd, see https://anaconda.org/anaconda/xlrd (installation via pip also available</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_excel(<span class="st">"Concrete_Data.xls"</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df.iloc[:,<span class="dv">8</span>]</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df.iloc[:,<span class="bu">list</span>(<span class="bu">range</span>(<span class="dv">8</span>))]</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>X.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="2">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Cement (component 1)(kg in a m^3 mixture)</th>
<th data-quarto-table-cell-role="th">Blast Furnace Slag (component 2)(kg in a m^3 mixture)</th>
<th data-quarto-table-cell-role="th">Fly Ash (component 3)(kg in a m^3 mixture)</th>
<th data-quarto-table-cell-role="th">Water (component 4)(kg in a m^3 mixture)</th>
<th data-quarto-table-cell-role="th">Superplasticizer (component 5)(kg in a m^3 mixture)</th>
<th data-quarto-table-cell-role="th">Coarse Aggregate (component 6)(kg in a m^3 mixture)</th>
<th data-quarto-table-cell-role="th">Fine Aggregate (component 7)(kg in a m^3 mixture)</th>
<th data-quarto-table-cell-role="th">Age (day)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>540.0</td>
<td>0.0</td>
<td>0.0</td>
<td>162.0</td>
<td>2.5</td>
<td>1040.0</td>
<td>676.0</td>
<td>28</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>540.0</td>
<td>0.0</td>
<td>0.0</td>
<td>162.0</td>
<td>2.5</td>
<td>1055.0</td>
<td>676.0</td>
<td>28</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>332.5</td>
<td>142.5</td>
<td>0.0</td>
<td>228.0</td>
<td>0.0</td>
<td>932.0</td>
<td>594.0</td>
<td>270</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>332.5</td>
<td>142.5</td>
<td>0.0</td>
<td>228.0</td>
<td>0.0</td>
<td>932.0</td>
<td>594.0</td>
<td>365</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>198.6</td>
<td>132.4</td>
<td>0.0</td>
<td>192.0</td>
<td>0.0</td>
<td>978.4</td>
<td>825.5</td>
<td>360</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>X.describe()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Cement (component 1)(kg in a m^3 mixture)</th>
<th data-quarto-table-cell-role="th">Blast Furnace Slag (component 2)(kg in a m^3 mixture)</th>
<th data-quarto-table-cell-role="th">Fly Ash (component 3)(kg in a m^3 mixture)</th>
<th data-quarto-table-cell-role="th">Water (component 4)(kg in a m^3 mixture)</th>
<th data-quarto-table-cell-role="th">Superplasticizer (component 5)(kg in a m^3 mixture)</th>
<th data-quarto-table-cell-role="th">Coarse Aggregate (component 6)(kg in a m^3 mixture)</th>
<th data-quarto-table-cell-role="th">Fine Aggregate (component 7)(kg in a m^3 mixture)</th>
<th data-quarto-table-cell-role="th">Age (day)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">count</td>
<td>1030.000000</td>
<td>1030.000000</td>
<td>1030.000000</td>
<td>1030.000000</td>
<td>1030.000000</td>
<td>1030.000000</td>
<td>1030.000000</td>
<td>1030.000000</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">mean</td>
<td>281.165631</td>
<td>73.895485</td>
<td>54.187136</td>
<td>181.566359</td>
<td>6.203112</td>
<td>972.918592</td>
<td>773.578883</td>
<td>45.662136</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">std</td>
<td>104.507142</td>
<td>86.279104</td>
<td>63.996469</td>
<td>21.355567</td>
<td>5.973492</td>
<td>77.753818</td>
<td>80.175427</td>
<td>63.169912</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">min</td>
<td>102.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>121.750000</td>
<td>0.000000</td>
<td>801.000000</td>
<td>594.000000</td>
<td>1.000000</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">25%</td>
<td>192.375000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>164.900000</td>
<td>0.000000</td>
<td>932.000000</td>
<td>730.950000</td>
<td>7.000000</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">50%</td>
<td>272.900000</td>
<td>22.000000</td>
<td>0.000000</td>
<td>185.000000</td>
<td>6.350000</td>
<td>968.000000</td>
<td>779.510000</td>
<td>28.000000</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">75%</td>
<td>350.000000</td>
<td>142.950000</td>
<td>118.270000</td>
<td>192.000000</td>
<td>10.160000</td>
<td>1029.400000</td>
<td>824.000000</td>
<td>56.000000</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">max</td>
<td>540.000000</td>
<td>359.400000</td>
<td>200.100000</td>
<td>247.000000</td>
<td>32.200000</td>
<td>1145.000000</td>
<td>992.600000</td>
<td>365.000000</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>All features are continuous variables, and we don’t have any missing value. However the ranges are very different. We will therefore standardize all columns in the input data.</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>X<span class="op">=</span>(X<span class="op">-</span>X.mean())<span class="op">/</span>X.std()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="preparation-of-datasets" class="level3">
<h3 class="anchored" data-anchor-id="preparation-of-datasets">Preparation of datasets</h3>
<p>We plan to utilize four algorithms, optimize their hyperparameters, and perform a comparative analysis. To enable a fair comparison, we require a set for comparison purposes. Ideally, each algorithm should also have distinct training and testing sets, but this would significantly reduce the data available for training the models. Consequently, we will employ cross-validation within the non-comparison set to train the models and fine-tune the hyperparameters.</p>
<p>As we intend to apply cross-validation to various models, it’s imperative that the cross-validation process remains consistent across all of them. Thus, we will establish the cross-validation splits in advance and use the same splits for each algorithm.</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split, cross_val_score</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> model_selection  </span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>seed <span class="op">=</span> <span class="dv">420</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>n_splits <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>X0, X_compare, y0, y_compare <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.25</span>, random_state<span class="op">=</span>seed)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>kfold <span class="op">=</span> model_selection.KFold(n_splits<span class="op">=</span>n_splits, shuffle<span class="op">=</span><span class="va">True</span>, random_state<span class="op">=</span>seed)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="using-different-models" class="level2">
<h2 class="anchored" data-anchor-id="using-different-models">Using different models</h2>
<p>All the models we will use here are ensemble methods in which weak learners, which are decision trees, have their output combined in order to produce a so-called strong learner which will give us a prediction. All these models differ in the way the weak learners are constructed, and the way their contributions are combined.</p>
<section id="bagging" class="level3">
<h3 class="anchored" data-anchor-id="bagging">Bagging</h3>
<p>Bagging, which stands for bootstrap aggregating, is a method in which we will choose the individual learners as decision trees. The training set is sampled with replacement (bootstrap), and is used to construct a decision tree. In a BaggingRegressor model, the output from the individual learners is average to produce the full model’s output.</p>
<p>There are two hyperparameters for a bagging model: the number of weak learners to introduce in the model and the size of the bootstrap samples to use to construct each of them.</p>
<div class="cell" data-scrolled="true" data-execution_count="6">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> BaggingRegressor</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>grid <span class="op">=</span> <span class="bu">dict</span>()</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>grid[<span class="st">'n_estimators'</span>] <span class="op">=</span> [<span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">40</span>, <span class="dv">60</span>, <span class="dv">80</span>, <span class="dv">120</span>, <span class="dv">150</span>, <span class="dv">200</span>, <span class="dv">300</span>, <span class="dv">400</span>, <span class="dv">600</span>, <span class="dv">800</span>]</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>grid[<span class="st">'max_samples'</span>] <span class="op">=</span> [<span class="fl">0.1</span><span class="op">*</span>i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">11</span>)]</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>mses <span class="op">=</span> np.zeros((<span class="bu">len</span>(grid[<span class="st">"n_estimators"</span>]), <span class="bu">len</span>(grid[<span class="st">"max_samples"</span>])))</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, n_estimators <span class="kw">in</span> <span class="bu">enumerate</span>(grid[<span class="st">"n_estimators"</span>]):</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j, max_samples <span class="kw">in</span> <span class="bu">enumerate</span>(grid[<span class="st">"max_samples"</span>]):</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>        <span class="co">#Making explicit that we are using Bagging algorithm</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>        model_bagging <span class="op">=</span> BaggingRegressor(bootstrap<span class="op">=</span><span class="va">True</span>, bootstrap_features<span class="op">=</span><span class="va">False</span>) </span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>        mses[i,j] <span class="op">=</span> <span class="bu">sum</span>(cross_val_score(</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>            model_bagging, X0, y0, cv<span class="op">=</span>kfold, n_jobs<span class="op">=-</span><span class="dv">1</span>, scoring<span class="op">=</span><span class="st">'neg_mean_squared_error'</span>)) <span class="op">/</span> n_splits</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">"figure.figsize"</span>] <span class="op">=</span> (<span class="dv">8</span>, <span class="dv">3</span>)<span class="op">;</span></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">'axes.grid'</span>] <span class="op">=</span> <span class="va">False</span> </span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure()</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> fig.add_subplot(<span class="dv">111</span>)</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>cax <span class="op">=</span> ax.matshow(mses.T, interpolation<span class="op">=</span><span class="st">'nearest'</span>, cmap<span class="op">=</span><span class="st">'Greys'</span>)</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>fig.colorbar(cax)</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>xaxis <span class="op">=</span> np.arange(<span class="bu">len</span>(grid[<span class="st">"n_estimators"</span>]))</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>yaxis <span class="op">=</span> np.arange(<span class="bu">len</span>(grid[<span class="st">"max_samples"</span>]))</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>ax.set_xticks(xaxis)</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>ax.set_yticks(yaxis)</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>ax.set_xticklabels(grid[<span class="st">"n_estimators"</span>])</span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>ax.set_yticklabels([<span class="ss">f"</span><span class="sc">{</span><span class="fl">0.1</span><span class="op">*</span>i<span class="sc">:.1f}</span><span class="ss">"</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">11</span>)])</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">"Max samples"</span>)</span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"# Estimators"</span>)</span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">"Performance of Bagging with different hyperparameters"</span>)<span class="op">;</span></span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="Hyperparameter_selection_files/figure-html/cell-7-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>We used a grid search to try and find the best hyperparameters for bagging. Since there are only two of them, we can visualize them with a heatmap. We represented it it above, hoping to uncover a meaningful pattern. Unfortunately, no such relationship appears to surface. It appears that there is a complex relationship between the hyperparameters and the cost function. Consequently, it proves challenging to propose a straightforward guideline for selecting suitable initial hyperparameter values for a bagging algorithm.</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>hyp_argmax <span class="op">=</span> np.unravel_index(np.argmax(mses), mses.shape)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>bag_params <span class="op">=</span> {<span class="st">"n_estimators"</span>: grid[<span class="st">'n_estimators'</span>][hyp_argmax[<span class="dv">0</span>]],</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>              <span class="st">"max_samples"</span>: grid[<span class="st">'max_samples'</span>][hyp_argmax[<span class="dv">1</span>]]}</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Best hyperparameters: </span><span class="ch">\n\t</span><span class="ss">n_estimators = </span><span class="sc">{</span>bag_params[<span class="st">'n_estimators'</span>]<span class="sc">}</span><span class="ch">\n\</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="ch">\t</span><span class="ss">max_samples = </span><span class="sc">{</span>bag_params[<span class="st">'max_samples'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Best mean squared error: </span><span class="sc">{</span><span class="op">-</span>mses[hyp_argmax]<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Best hyperparameters: 
    n_estimators = 800
    max_samples = 0.6000000000000001
Best mean squared error: 32.059</code></pre>
</div>
</div>
<p>Examining the individual impact of each parameter in isolation wouldn’t yield meaningful insights in this context, since there is no homogeneity in the search space. Our conclusions regarding the effects of increasing or decreasing a hyperparameter would change arbitrarily.</p>
</section>
<section id="random-forests" class="level3">
<h3 class="anchored" data-anchor-id="random-forests">Random Forests</h3>
<p>Random forests are also an ensemble method in which the individual learners are decision trees. In random forests, the totality of the training set but only a subset of the features are used to train each decision tree. The final estimation again is produced by averaging the predictions of each tree.</p>
<p>In random forests, there are a number of hyperparameters. We decided do find the best combination of four of them. The number of estimators to use in the model is the first one, and is common to all methods we will use in this document. The number of features to use is the first characteristic of random forests. For the construction of each tree we will also try to find optimal values for its maximum depth as well as the number of points to be present in a group after division for this group to be considered a leaf.</p>
<p>Hyperparameter optimization through grid search is acknowledged to be highly inefficient. A commonly used alternative search method is randomized search, which involves testing random hyperparameter combinations and selecting the best-performing one. We start with the standard grid-search:</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestRegressor</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> GridSearchCV, RandomizedSearchCV</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>grid <span class="op">=</span> <span class="bu">dict</span>()</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>grid[<span class="st">'n_estimators'</span>] <span class="op">=</span> [<span class="dv">10</span>, <span class="dv">25</span>, <span class="dv">50</span>, <span class="dv">100</span>, <span class="dv">200</span>, <span class="dv">400</span>, <span class="dv">800</span>]</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>grid[<span class="st">'max_features'</span>] <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(<span class="dv">2</span>, <span class="dv">9</span>))</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>grid[<span class="st">'max_depth'</span>] <span class="op">=</span> [<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">8</span>, <span class="dv">12</span>, <span class="va">None</span>]</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>grid[<span class="st">'min_samples_leaf'</span>] <span class="op">=</span> [<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>]</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>model_rf <span class="op">=</span> RandomForestRegressor()</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>tic <span class="op">=</span> time.time()</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>grid_search <span class="op">=</span> GridSearchCV(estimator<span class="op">=</span>model_rf, param_grid<span class="op">=</span>grid, n_jobs<span class="op">=-</span><span class="dv">1</span>, cv<span class="op">=</span>kfold, scoring<span class="op">=</span><span class="st">'neg_mean_squared_error'</span>)</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>grid_result <span class="op">=</span> grid_search.fit(X0, y0)</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Grid search performed in </span><span class="sc">{</span>time<span class="sc">.</span>time() <span class="op">-</span> tic<span class="sc">:.0f}</span><span class="ss">s"</span>)</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Best: </span><span class="sc">%f</span><span class="st"> using:"</span> <span class="op">%</span> (<span class="op">-</span>grid_result.best_score_))</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k, v <span class="kw">in</span> grid_result.best_params_.items():</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\t</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>v<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a><span class="co"># summarize all scores that were evaluated</span></span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>means <span class="op">=</span> grid_result.cv_results_[<span class="st">'mean_test_score'</span>]</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>stds <span class="op">=</span> grid_result.cv_results_[<span class="st">'std_test_score'</span>]</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>params <span class="op">=</span> grid_result.cv_results_[<span class="st">'params'</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Grid search performed in 86s
Best: 30.909331 using:
    max_depth: None
    max_features: 6
    min_samples_leaf: 2
    n_estimators: 100</code></pre>
</div>
</div>
<p>Random search being a very popular alternative to grid search, this is the approach we try next:</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>model_rf <span class="op">=</span> RandomForestRegressor()</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>tic <span class="op">=</span> time.time()</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>rando_search <span class="op">=</span> RandomizedSearchCV(estimator<span class="op">=</span>model_rf, param_distributions<span class="op">=</span>grid, cv<span class="op">=</span>kfold,</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>                                  n_iter<span class="op">=</span><span class="dv">500</span>, n_jobs<span class="op">=-</span><span class="dv">1</span>, scoring<span class="op">=</span><span class="st">'neg_mean_squared_error'</span>)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>rando_result <span class="op">=</span> rando_search.fit(X0, y0)</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Randomized search performed in </span><span class="sc">{</span>time<span class="sc">.</span>time() <span class="op">-</span> tic<span class="sc">:.0f}</span><span class="ss">s"</span>)</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Best: </span><span class="sc">%f</span><span class="st"> using:"</span> <span class="op">%</span> (<span class="op">-</span>rando_result.best_score_))</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k, v <span class="kw">in</span> rando_result.best_params_.items():</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\t</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>v<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a><span class="co"># summarize all scores that were evaluated</span></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>means <span class="op">=</span> rando_result.cv_results_[<span class="st">'mean_test_score'</span>]</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>stds <span class="op">=</span> rando_result.cv_results_[<span class="st">'std_test_score'</span>]</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>params <span class="op">=</span> rando_result.cv_results_[<span class="st">'params'</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Randomized search performed in 52s
Best: 31.116653 using:
    n_estimators: 400
    min_samples_leaf: 2
    max_features: 7
    max_depth: None</code></pre>
</div>
</div>
<p>Another intriguing avenue for hyperparameter optimization is the utilization of stochastic optimization algorithms. For the sake of broadening our experience with this category of algorithms, and since we already used genetic algorithms in a previous project, we have chosen to explore particle swarm optimization (PSO), which belongs to the category of swarm intelligence algorithms. PSO operates on a straightforward principle. Initially, a set of potential solutions is established and scattered across the search space, each having its own position and velocity. At each time-step, these solutions adjust their positions based on their velocities and update their velocities, which can be influenced by their “best seen position” or the global “best seen position” (or both) with respect to a function of their coordinates. In this context, the function of their coordinates can be a classifier, and the coordinates represent hyperparameters. The evolution of the algorithm can be likened to a swarm that gradually converges and explores around various local solutions. One distinctive feature of PSO, as opposed to GA, is that PSO particles maintain a record of their best previous positions, ensuring that the best solution discovered so far can only be enhanced over iterations.</p>
<p>Given that the <em>deap</em> package can also be employed to implement PSO, we will use it for this purpose.</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> operator</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> deap <span class="im">import</span> base, creator, tools</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>grid <span class="op">=</span> <span class="bu">dict</span>()</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>grid[<span class="st">'n_estimators'</span>] <span class="op">=</span> [<span class="dv">10</span>, <span class="dv">25</span>, <span class="dv">50</span>, <span class="dv">100</span>, <span class="dv">200</span>, <span class="dv">400</span>, <span class="dv">800</span>]</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>grid[<span class="st">'max_features'</span>] <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(<span class="dv">2</span>, <span class="dv">9</span>))</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>grid[<span class="st">'max_depth'</span>] <span class="op">=</span> [<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">8</span>, <span class="dv">12</span>]</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>grid[<span class="st">'min_samples_leaf'</span>] <span class="op">=</span> [<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>]</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Creating the particles</span></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>creator.create(<span class="st">"FitnessMin"</span>, base.Fitness, weights<span class="op">=</span>(<span class="op">-</span><span class="fl">1.0</span>,))</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>creator.create(<span class="st">"Particle"</span>, <span class="bu">list</span>, fitness<span class="op">=</span>creator.FitnessMin, speed<span class="op">=</span><span class="bu">list</span>, </span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>    smin<span class="op">=</span><span class="va">None</span>, smax<span class="op">=</span><span class="va">None</span>, best<span class="op">=</span><span class="va">None</span>, grid<span class="op">=</span>grid)</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate(size, smin, smax):</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Particles will have a number between 0 and 1 which will be converted into the parameters</span></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># at evaluation time</span></span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>    part <span class="op">=</span> creator.Particle(random.random() <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(size)) </span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>    part.speed <span class="op">=</span> [random.uniform(smin, smax) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(size)]</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>    part.smin <span class="op">=</span> smin</span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>    part.smax <span class="op">=</span> smax</span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> part</span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> scale_back(x, label, <span class="bu">type</span><span class="op">=</span><span class="st">"linear"</span>):</span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">type</span><span class="op">==</span><span class="st">"linear"</span>:</span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> math.floor(grid[label][<span class="dv">0</span>] <span class="op">+</span> x<span class="op">*</span>(grid[label][<span class="op">-</span><span class="dv">1</span>] <span class="op">-</span> grid[label][<span class="dv">0</span>]))</span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># For values where relative augmentations make more sense</span></span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">type</span><span class="op">==</span><span class="st">"log"</span>:</span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> math.floor(grid[label][<span class="op">-</span><span class="dv">1</span>]<span class="op">**</span>x <span class="op">+</span> grid[label][<span class="dv">0</span>] <span class="op">-</span> <span class="dv">1</span>)</span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> evaluate_rf(part):</span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Extracting particle parameters</span></span>
<span id="cb13-38"><a href="#cb13-38" aria-hidden="true" tabindex="-1"></a>    n_estimators <span class="op">=</span> scale_back(part[<span class="dv">0</span>], <span class="st">"n_estimators"</span>, <span class="bu">type</span><span class="op">=</span><span class="st">"log"</span>)</span>
<span id="cb13-39"><a href="#cb13-39" aria-hidden="true" tabindex="-1"></a>    max_features <span class="op">=</span> scale_back(part[<span class="dv">1</span>], <span class="st">"max_features"</span>)</span>
<span id="cb13-40"><a href="#cb13-40" aria-hidden="true" tabindex="-1"></a>    max_depth <span class="op">=</span> scale_back(part[<span class="dv">2</span>], <span class="st">"max_depth"</span>)</span>
<span id="cb13-41"><a href="#cb13-41" aria-hidden="true" tabindex="-1"></a>    min_samples_leaf <span class="op">=</span> scale_back(part[<span class="dv">3</span>], <span class="st">"min_samples_leaf"</span>)</span>
<span id="cb13-42"><a href="#cb13-42" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Evaluating the model</span></span>
<span id="cb13-43"><a href="#cb13-43" aria-hidden="true" tabindex="-1"></a>    model_rf <span class="op">=</span> RandomForestRegressor()</span>
<span id="cb13-44"><a href="#cb13-44" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">sum</span>(cross_val_score(model_rf, X0, y0, cv<span class="op">=</span>kfold, </span>
<span id="cb13-45"><a href="#cb13-45" aria-hidden="true" tabindex="-1"></a>                               scoring<span class="op">=</span><span class="st">'neg_mean_squared_error'</span>, n_jobs<span class="op">=-</span><span class="dv">1</span>))<span class="op">/</span>n_splits,</span>
<span id="cb13-46"><a href="#cb13-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-47"><a href="#cb13-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-48"><a href="#cb13-48" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> updateParticle(part, best, phi1, phi2):</span>
<span id="cb13-49"><a href="#cb13-49" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Computing speed towards personal best and global best positions</span></span>
<span id="cb13-50"><a href="#cb13-50" aria-hidden="true" tabindex="-1"></a>    u1 <span class="op">=</span> (random.uniform(<span class="dv">0</span>, phi1) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(part)))</span>
<span id="cb13-51"><a href="#cb13-51" aria-hidden="true" tabindex="-1"></a>    u2 <span class="op">=</span> (random.uniform(<span class="dv">0</span>, phi2) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(part)))</span>
<span id="cb13-52"><a href="#cb13-52" aria-hidden="true" tabindex="-1"></a>    v_u1 <span class="op">=</span> <span class="bu">map</span>(operator.mul, u1, <span class="bu">map</span>(operator.sub, part.best, part))</span>
<span id="cb13-53"><a href="#cb13-53" aria-hidden="true" tabindex="-1"></a>    v_u2 <span class="op">=</span> <span class="bu">map</span>(operator.mul, u2, <span class="bu">map</span>(operator.sub, best, part))</span>
<span id="cb13-54"><a href="#cb13-54" aria-hidden="true" tabindex="-1"></a>    part.speed <span class="op">=</span> <span class="bu">list</span>(<span class="bu">map</span>(operator.add, part.speed, <span class="bu">map</span>(operator.add, v_u1, v_u2)))</span>
<span id="cb13-55"><a href="#cb13-55" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, speed <span class="kw">in</span> <span class="bu">enumerate</span>(part.speed):</span>
<span id="cb13-56"><a href="#cb13-56" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">abs</span>(speed) <span class="op">&lt;</span> part.smin:</span>
<span id="cb13-57"><a href="#cb13-57" aria-hidden="true" tabindex="-1"></a>            part.speed[i] <span class="op">=</span> math.copysign(part.smin, speed)</span>
<span id="cb13-58"><a href="#cb13-58" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> <span class="bu">abs</span>(speed) <span class="op">&gt;</span> part.smax:</span>
<span id="cb13-59"><a href="#cb13-59" aria-hidden="true" tabindex="-1"></a>            part.speed[i] <span class="op">=</span> math.copysign(part.smax, speed)</span>
<span id="cb13-60"><a href="#cb13-60" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Moving the particle, making sure the coordinates stay in the 0-1 range</span></span>
<span id="cb13-61"><a href="#cb13-61" aria-hidden="true" tabindex="-1"></a>    part[:] <span class="op">=</span> <span class="bu">list</span>(<span class="bu">map</span>(operator.add, part, part.speed))</span>
<span id="cb13-62"><a href="#cb13-62" aria-hidden="true" tabindex="-1"></a>    part[:] <span class="op">=</span> <span class="bu">list</span>(<span class="bu">map</span>(<span class="kw">lambda</span> x: <span class="bu">min</span>(<span class="bu">max</span>(x, <span class="dv">0</span>), <span class="dv">1</span>), part))</span>
<span id="cb13-63"><a href="#cb13-63" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Updating the speed boundaries to help the particles to slow down and converge closer to minimum</span></span>
<span id="cb13-64"><a href="#cb13-64" aria-hidden="true" tabindex="-1"></a>    part.smin <span class="op">*=</span> <span class="fl">0.95</span></span>
<span id="cb13-65"><a href="#cb13-65" aria-hidden="true" tabindex="-1"></a>    part.smax <span class="op">*=</span> <span class="fl">0.95</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>toolbox <span class="op">=</span> base.Toolbox()</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>toolbox.register(<span class="st">"particle"</span>, generate, size<span class="op">=</span><span class="dv">4</span>, smin<span class="op">=-</span><span class="fl">0.3</span>, smax<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>toolbox.register(<span class="st">"population"</span>, tools.initRepeat, <span class="bu">list</span>, toolbox.particle)</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>toolbox.register(<span class="st">"update"</span>, updateParticle, phi1<span class="op">=</span><span class="fl">2.0</span>, phi2<span class="op">=</span><span class="fl">2.0</span>)</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>toolbox.register(<span class="st">"evaluate"</span>, evaluate_rf)</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>pop <span class="op">=</span> toolbox.population(n<span class="op">=</span><span class="dv">15</span>)</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>stats <span class="op">=</span> tools.Statistics(<span class="kw">lambda</span> ind: ind.fitness.values)</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>stats.register(<span class="st">"avg"</span>, np.mean)</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>stats.register(<span class="st">"std"</span>, np.std)</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>stats.register(<span class="st">"min"</span>, np.<span class="bu">min</span>)</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>stats.register(<span class="st">"max"</span>, np.<span class="bu">max</span>)</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>logbook <span class="op">=</span> tools.Logbook()</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>logbook.header <span class="op">=</span> [<span class="st">"gen"</span>, <span class="st">"evals"</span>] <span class="op">+</span> stats.fields</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>GEN <span class="op">=</span> <span class="dv">25</span></span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>best <span class="op">=</span> <span class="va">None</span></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>tic <span class="op">=</span> time.time()</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> g <span class="kw">in</span> <span class="bu">range</span>(GEN):</span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> part <span class="kw">in</span> pop:</span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>        part.fitness.values <span class="op">=</span> toolbox.evaluate(part)</span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> part.best <span class="kw">or</span> part.best.fitness <span class="op">&gt;</span> part.fitness:</span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>            part.best <span class="op">=</span> creator.Particle(part)</span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a>            part.best.fitness.values <span class="op">=</span> part.fitness.values</span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> best <span class="kw">or</span> best.fitness <span class="op">&gt;</span> part.fitness:</span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a>            best <span class="op">=</span> creator.Particle(part)</span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a>            best.fitness.values <span class="op">=</span> part.fitness.values</span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> part <span class="kw">in</span> pop:</span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a>        toolbox.update(part, best)</span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Gather all the fitnesses in one list and print the stats</span></span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a>    logbook.record(gen<span class="op">=</span>g, evals<span class="op">=</span><span class="bu">len</span>(pop), <span class="op">**</span>stats.<span class="bu">compile</span>(pop))</span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(logbook.stream)</span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Optimized in </span><span class="sc">{</span>time<span class="sc">.</span>time() <span class="op">-</span> tic<span class="sc">:.0f}</span><span class="ss">s"</span>)</span>
<span id="cb14-38"><a href="#cb14-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-39"><a href="#cb14-39" aria-hidden="true" tabindex="-1"></a>rf_params <span class="op">=</span> [scale_back(best[<span class="dv">0</span>], <span class="st">"n_estimators"</span>, <span class="bu">type</span><span class="op">=</span><span class="st">"log"</span>)]</span>
<span id="cb14-40"><a href="#cb14-40" aria-hidden="true" tabindex="-1"></a>rf_params <span class="op">=</span> rf_params <span class="op">+</span> [scale_back(e, lab) <span class="cf">for</span> lab, e <span class="kw">in</span> <span class="bu">zip</span>((<span class="st">"max_features"</span>, <span class="st">"max_depth"</span>, <span class="st">"min_samples_leaf"</span>), best[<span class="dv">1</span>:])]</span>
<span id="cb14-41"><a href="#cb14-41" aria-hidden="true" tabindex="-1"></a>rf_params <span class="op">=</span> {lab: e <span class="cf">for</span> lab, e <span class="kw">in</span> <span class="bu">zip</span>((<span class="st">"n_estimators"</span>, <span class="st">"max_features"</span>, <span class="st">"max_depth"</span>, <span class="st">"min_samples_leaf"</span>), rf_params)}</span>
<span id="cb14-42"><a href="#cb14-42" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Best parameters:"</span>)</span>
<span id="cb14-43"><a href="#cb14-43" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k, v <span class="kw">in</span> rf_params.items(): <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\t</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>v<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb14-44"><a href="#cb14-44" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"With associated mean squared error:"</span>, <span class="op">-</span>pd.DataFrame(logbook)[<span class="st">"max"</span>].<span class="bu">max</span>())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>gen evals   avg         std         min     max     
0   15      -30.2734    0.326345    -30.885 -29.7146
1   15      -30.3434    0.331606    -30.8303    -29.6707
2   15      -30.3069    0.482355    -31.2241    -29.2866
3   15      -30.4321    0.353858    -30.9683    -29.9771
4   15      -30.3563    0.319324    -30.9131    -29.8951
5   15      -30.4894    0.419406    -31.0863    -29.73  
6   15      -30.4424    0.229928    -30.8474    -29.9288
7   15      -30.3252    0.308457    -30.8596    -29.6557
8   15      -30.3162    0.476522    -31.0173    -29.3516
9   15      -30.3514    0.451328    -31.4528    -29.784 
10  15      -30.4655    0.409551    -31.218     -29.443 
11  15      -30.4177    0.340926    -31.1454    -29.9083
12  15      -30.3591    0.457771    -31.2102    -29.7366
13  15      -30.3902    0.315966    -31.2163    -29.969 
14  15      -30.3761    0.40547     -31.291     -29.6716
15  15      -30.4408    0.340373    -30.9258    -29.6514
16  15      -30.27      0.341       -30.8348    -29.7053
17  15      -30.3594    0.26875     -31.0837    -30.0437
18  15      -30.4204    0.368323    -31.1995    -29.94  
19  15      -30.4316    0.271415    -30.8547    -29.8989
20  15      -30.3871    0.350388    -31.0799    -29.8885
21  15      -30.3095    0.369622    -30.9278    -29.3503
22  15      -30.3458    0.319224    -30.843     -29.7142
23  15      -30.369     0.369979    -30.9412    -29.8144
24  15      -30.4855    0.546448    -31.5883    -29.5479
Optimized in 56s
Best parameters:
    n_estimators: 75
    max_features: 6
    max_depth: 8
    min_samples_leaf: 2
With associated mean squared error: 29.286558386699017</code></pre>
</div>
</div>
<p>PSO performs the best: it is both quite fast to find good regions of the search space and spends a lot of time exploring around the local minimas. As a result, not many iterations are necessary and the algorithm becomes quite fast. It also outperformed grid-search, which is somewhat counterintuitive at first as we could expect grid search to behave kind of like a brute-force search. It happens that the exploration performed by grid search is actually very inefficient. This result is particularly impressive as we used close to the most naive implementation of PSO. the only thing we added to the most naive approach is a decay factor on the speed of particles, so that they gravitate closer and closer to the local minimas as the generation increases. We might consider researching more on this approach in the future.</p>
<section id="impact-of-hyperparameters" class="level4">
<h4 class="anchored" data-anchor-id="impact-of-hyperparameters">Impact of hyperparameters</h4>
<p>In order to assess how the hyperparameters impact the fitting of the model, we will make a few plots. We will make the hyperparameters vary from the optimal position. This is because we can’t be sure if the impact of hyperparameters will be the same if we are far from a good model, so we thought that the best <em>place</em> to assess the impact was close to the optimal values.</p>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_models_rf(choice):</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>    n_best <span class="op">=</span> rf_params[<span class="st">'n_estimators'</span>]</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>    ft_best <span class="op">=</span> rf_params[<span class="st">'max_features'</span>]</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>    depth_best <span class="op">=</span> rf_params[<span class="st">'max_depth'</span>]</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>    msl_best <span class="op">=</span> rf_params[<span class="st">'min_samples_leaf'</span>]</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>    models <span class="op">=</span> <span class="bu">dict</span>()</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>    n_trees <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(<span class="dv">10</span>,n_best,<span class="dv">10</span>)) <span class="op">+</span> <span class="bu">list</span>(<span class="bu">range</span>(n_best,<span class="dv">1000</span>,<span class="dv">100</span>)) <span class="op">+</span> <span class="bu">list</span>(<span class="bu">range</span>(<span class="dv">1000</span>,<span class="dv">5000</span>,<span class="dv">1000</span>))</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> choice <span class="op">==</span> <span class="st">'estimators'</span> :</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> n <span class="kw">in</span> n_trees:</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>            models[<span class="bu">str</span>(n)] <span class="op">=</span> RandomForestRegressor(n_estimators<span class="op">=</span>n, min_samples_leaf<span class="op">=</span> msl_best ,max_depth <span class="op">=</span> depth_best, max_features<span class="op">=</span>ft_best )</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> choice <span class="op">==</span> <span class="st">'msl'</span> :</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,<span class="dv">5</span>):</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>            key <span class="op">=</span> <span class="st">'</span><span class="sc">%.4f</span><span class="st">'</span> <span class="op">%</span> i</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>            models[key] <span class="op">=</span> RandomForestRegressor(min_samples_leaf<span class="op">=</span> msl_best , n_estimators<span class="op">=</span> n_best, max_depth <span class="op">=</span> depth_best , max_features<span class="op">=</span>ft_best  )</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> choice <span class="op">==</span> <span class="st">'depth'</span> :</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,<span class="dv">11</span>):</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>            models[<span class="bu">str</span>(i)] <span class="op">=</span> RandomForestRegressor(max_depth<span class="op">=</span>i,min_samples_leaf<span class="op">=</span> msl_best ,n_estimators<span class="op">=</span> n_best , max_features<span class="op">=</span>ft_best )</span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> choice <span class="op">==</span> <span class="st">'features'</span> :</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> [<span class="va">None</span>, <span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">6</span>]:</span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a>            models[<span class="bu">str</span>(i)] <span class="op">=</span> RandomForestRegressor(max_features<span class="op">=</span>i ,max_depth<span class="op">=</span>depth_best,min_samples_leaf<span class="op">=</span> msl_best ,n_estimators<span class="op">=</span> n_best)</span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> models</span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> evaluate_model(model, X,y):</span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a>    cv <span class="op">=</span> KFold(n_splits<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a>    mse_scorer <span class="op">=</span> make_scorer(mean_squared_error, greater_is_better<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> <span class="op">-</span>cross_val_score(model, X, y, scoring<span class="op">=</span> mse_scorer, cv<span class="op">=</span>cv, n_jobs<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> scores</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split </span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> [<span class="st">'depth'</span>,<span class="st">'features'</span>,<span class="st">'msl'</span>,<span class="st">'estimators'</span>]:</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>    models <span class="op">=</span> get_models_rf(i)</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>    results, names <span class="op">=</span> <span class="bu">list</span>(), <span class="bu">list</span>()</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> name, model <span class="kw">in</span> models.items():</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>        scores <span class="op">=</span> evaluate_model(model, X,y)</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>        results.append(scores)</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>        names.append(name)</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>        <span class="co">#print('&gt;%s %.3f' % (name, np.mean(scores)))</span></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>    plt.boxplot(results, labels<span class="op">=</span>names, showmeans<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="ss">f'Mean Squared Error depending on the value of the </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="Hyperparameter_selection_files/figure-html/cell-14-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="Hyperparameter_selection_files/figure-html/cell-14-output-2.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="Hyperparameter_selection_files/figure-html/cell-14-output-3.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="Hyperparameter_selection_files/figure-html/cell-14-output-4.png" class="img-fluid"></p>
</div>
</div>
<p>For each parameter, we take different value of it and compute the mean squared error on the model we got, making sure that the other parameters are set to the best value we found before.<br>
We can see that the bigger the depth is, the better the model is performing. This is not true for other parameters. In particular, increasing min sample leafs seems to deteriorate the model, indicating that random forests just perform better if they are allowed to make <em>smaller</em> leaves.<br>
When we play with the number of estimators, we can see that at a time, performances are becoming more stable, and, even if we increase the number of estimator, MSE isn’t necessary lower.</p>
</section>
</section>
<section id="adaboost" class="level3">
<h3 class="anchored" data-anchor-id="adaboost">AdaBoost</h3>
<p>AdaBoost, standing for adaptive boosting, is another ensemble learning method. The particularity of adaboost is the method by which the weak learners are constructed: each point in the dataset is assigned a weight, uniform at first. Then each time a weak learner is trained, the weights of the points are updated in order to give more weight to the points that are badly predicted by the model and less weight to those that are well predicted. The idea is that subsequent weak learners will be incentivized to learn patterns that have not been already learnt by the previous weak learners, hopefully improving the quality of the model as a whole.</p>
<section id="optimization-of-hyperparameters" class="level4">
<h4 class="anchored" data-anchor-id="optimization-of-hyperparameters">Optimization of hyperparameters</h4>
<p>In AdaBoost, the two most important hyperparameters are the number of weak learners to use and the learning rate, which translates as how much to reweight the points after each weak learner has been constructed. We will optimize over them by using a grid search approach, as it was the approach suggested for this document.</p>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> AdaBoostRegressor</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>grid <span class="op">=</span> <span class="bu">dict</span>()</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>grid[<span class="st">'n_estimators'</span>] <span class="op">=</span> [<span class="dv">10</span>, <span class="dv">25</span>, <span class="dv">50</span>, <span class="dv">100</span>, <span class="dv">200</span>, <span class="dv">400</span>, <span class="dv">800</span>]</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>grid[<span class="st">'learning_rate'</span>] <span class="op">=</span> [<span class="dv">2</span><span class="op">**</span>i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="op">-</span><span class="dv">3</span>, <span class="dv">2</span>)]</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>mses_ada <span class="op">=</span> np.zeros((<span class="bu">len</span>(grid[<span class="st">"n_estimators"</span>]), <span class="bu">len</span>(grid[<span class="st">"learning_rate"</span>])))</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, n_estimators <span class="kw">in</span> <span class="bu">enumerate</span>(grid[<span class="st">"n_estimators"</span>]):</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j, learning_rate <span class="kw">in</span> <span class="bu">enumerate</span>(grid[<span class="st">"learning_rate"</span>]):</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>        model_ada <span class="op">=</span> AdaBoostRegressor(n_estimators<span class="op">=</span>n_estimators, learning_rate<span class="op">=</span>learning_rate) <span class="co">#Making explicit that we are using Bagging algorithm</span></span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>        mses_ada[i,j] <span class="op">=</span> <span class="bu">sum</span>(cross_val_score(</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>            model_ada, X0, y0, cv<span class="op">=</span>kfold, n_jobs<span class="op">=-</span><span class="dv">1</span>, scoring<span class="op">=</span><span class="st">'neg_mean_squared_error'</span>)) <span class="op">/</span> n_splits</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">"figure.figsize"</span>] <span class="op">=</span> (<span class="dv">8</span>, <span class="dv">4</span>)<span class="op">;</span></span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">'axes.grid'</span>] <span class="op">=</span> <span class="va">False</span> </span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure()</span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> fig.add_subplot(<span class="dv">111</span>)</span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a>cax <span class="op">=</span> ax.matshow(<span class="op">-</span>mses_ada.T, interpolation<span class="op">=</span><span class="st">'nearest'</span>, cmap<span class="op">=</span><span class="st">'Greys'</span>)</span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a>fig.colorbar(cax)</span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a>xaxis <span class="op">=</span> np.arange(<span class="bu">len</span>(grid[<span class="st">"n_estimators"</span>]))</span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a>yaxis <span class="op">=</span> np.arange(<span class="bu">len</span>(grid[<span class="st">"learning_rate"</span>]))</span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a>ax.set_xticks(xaxis)</span>
<span id="cb18-25"><a href="#cb18-25" aria-hidden="true" tabindex="-1"></a>ax.set_yticks(yaxis)</span>
<span id="cb18-26"><a href="#cb18-26" aria-hidden="true" tabindex="-1"></a>ax.set_xticklabels(grid[<span class="st">"n_estimators"</span>])</span>
<span id="cb18-27"><a href="#cb18-27" aria-hidden="true" tabindex="-1"></a>ax.set_yticklabels(grid[<span class="st">"learning_rate"</span>])</span>
<span id="cb18-28"><a href="#cb18-28" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">"Learning rate"</span>)</span>
<span id="cb18-29"><a href="#cb18-29" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"# Estimators"</span>)</span>
<span id="cb18-30"><a href="#cb18-30" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">"Performance of AdaBoost with different hyperparameters"</span>)<span class="op">;</span></span>
<span id="cb18-31"><a href="#cb18-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-32"><a href="#cb18-32" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="Hyperparameter_selection_files/figure-html/cell-15-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>hyp_argmax <span class="op">=</span> np.unravel_index(np.argmax(mses_ada), mses_ada.shape)</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>ada_params <span class="op">=</span> {<span class="st">"n_estimators"</span>: grid[<span class="st">'n_estimators'</span>][hyp_argmax[<span class="dv">0</span>]],</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>              <span class="st">"learning_rate"</span>: grid[<span class="st">'learning_rate'</span>][hyp_argmax[<span class="dv">1</span>]]}</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Best hyperparameters: </span><span class="ch">\n\t</span><span class="ss">n_estimators = </span><span class="sc">{</span>ada_params[<span class="st">'n_estimators'</span>]<span class="sc">}</span><span class="ch">\n\</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a><span class="ch">\t</span><span class="ss">learning_rate = </span><span class="sc">{</span>ada_params[<span class="st">'learning_rate'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Best mean squared error: </span><span class="sc">{</span><span class="op">-</span>mses_ada[hyp_argmax]<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Best hyperparameters: 
    n_estimators = 50
    learning_rate = 2
Best mean squared error: 61.469</code></pre>
</div>
</div>
</section>
<section id="impact-of-hyperparameters-1" class="level4">
<h4 class="anchored" data-anchor-id="impact-of-hyperparameters-1">Impact of hyperparameters</h4>
<p>From the above heatmap, we see quite well that using more weak learners in the model will perform better with a slow learning rate, and to the contrary a higher learning rate pairs best with less weak learners in the model. There also is a <em>sweet spot</em> somewhere as from the above, with the best performing pairs being close together at (400, 0.25) and (200, 0.5).</p>
<p>Unfortunately, it seems that adaboost doesn’t perform very well on this problem, with a mean squared error almost twice as high as other models.</p>
</section>
</section>
<section id="gradient-boosting" class="level3">
<h3 class="anchored" data-anchor-id="gradient-boosting">Gradient boosting</h3>
<p>Gradient boosting or GBM (gradient boosting models) is an extension of AdaBoost models. Its generalization comes from the fact that it allows optimization of an arbitrary differentiable loss function. At each step, the collective error given by the previously constructed weak learners is computed, and the next weak learner is constructed to minimize this residual.</p>
<section id="optimization-of-hyperparameters-1" class="level4">
<h4 class="anchored" data-anchor-id="optimization-of-hyperparameters-1">Optimization of hyperparameters</h4>
<p>For GBM we decided to optimize the number of weak learners in the model, the learning rate which again represents how much a new learner tries to correct the current residual, and since we are using decision trees as weak learners we also included their max depth. Once again, we will try to find the best set with a grid search.</p>
<div class="cell" data-scrolled="true" data-execution_count="16">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> GradientBoostingRegressor  </span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>grid <span class="op">=</span> <span class="bu">dict</span>()</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>grid[<span class="st">'n_estimators'</span>] <span class="op">=</span> [<span class="dv">5</span><span class="op">*</span>i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,<span class="dv">16</span>)]</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>grid[<span class="st">'learning_rate'</span>] <span class="op">=</span> [<span class="dv">2</span><span class="op">**</span>i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="op">-</span><span class="dv">7</span>, <span class="dv">4</span>)]</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>grid[<span class="st">'max_depth'</span>] <span class="op">=</span> [<span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">6</span>, <span class="dv">7</span>, <span class="dv">8</span>]</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>model_GB <span class="op">=</span> GradientBoostingRegressor()</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>tic <span class="op">=</span> time.time()</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>grid_search <span class="op">=</span> GridSearchCV(estimator<span class="op">=</span>model_GB, param_grid<span class="op">=</span>grid, </span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>                           n_jobs<span class="op">=-</span><span class="dv">1</span>, cv<span class="op">=</span>kfold, scoring<span class="op">=</span><span class="st">'neg_mean_squared_error'</span>)</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>GB_params <span class="op">=</span> grid_search.fit(X0, y0)</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Grid search performed in </span><span class="sc">{</span>time<span class="sc">.</span>time() <span class="op">-</span> tic<span class="sc">:.0f}</span><span class="ss">s"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Grid search performed in 19s</code></pre>
</div>
</div>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>GB_params.best_params_</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="17">
<pre><code>{'learning_rate': 0.25, 'max_depth': 5, 'n_estimators': 60}</code></pre>
</div>
</div>
<div class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>model_GB <span class="op">=</span> GradientBoostingRegressor(<span class="op">**</span>GB_params.best_params_)</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>CVmse <span class="op">=</span> cross_val_score(model_GB, X0, y0, cv<span class="op">=</span>kfold, scoring<span class="op">=</span><span class="st">'neg_mean_squared_error'</span>)</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Average mean squared error for GBM: </span><span class="sc">{</span><span class="op">-</span><span class="bu">sum</span>(CVmse)<span class="op">/</span>n_splits<span class="sc">:.2f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Average mean squared error for GBM: 23.48</code></pre>
</div>
</div>
<p>The first thing we observe right off the bat is how fast these algorithms were trained. This can be explained with the low number of estimators we used. We couldn’t use more than 80 as the training phase produced overflow errors. The other very promising thing is the low cross-validation error of the selected model. We will see in the last part if this model is robust to overfitting.</p>
</section>
<section id="impact-of-hyperparameters-2" class="level4">
<h4 class="anchored" data-anchor-id="impact-of-hyperparameters-2">Impact of hyperparameters</h4>
<div class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_models_GB(choice):</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>    n_best <span class="op">=</span> GB_params.best_params_[<span class="st">'n_estimators'</span>]</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>    rate_best <span class="op">=</span> GB_params.best_params_[<span class="st">'learning_rate'</span>]</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>    depth_best <span class="op">=</span> GB_params.best_params_[<span class="st">'max_depth'</span>]</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>    models <span class="op">=</span> <span class="bu">dict</span>()</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>    n_trees <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(<span class="dv">10</span>,n_best,<span class="dv">10</span>)) <span class="op">+</span> <span class="bu">list</span>(<span class="bu">range</span>(n_best,<span class="dv">1000</span>,<span class="dv">100</span>)) <span class="op">+</span> <span class="bu">list</span>(<span class="bu">range</span>(<span class="dv">1000</span>,<span class="dv">5000</span>,<span class="dv">1000</span>))</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> choice <span class="op">==</span> <span class="st">'estimators'</span> :</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> n <span class="kw">in</span> n_trees:</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>            models[<span class="bu">str</span>(n)] <span class="op">=</span> GradientBoostingRegressor(n_estimators<span class="op">=</span>n, learning_rate <span class="op">=</span> rate_best,max_depth <span class="op">=</span> depth_best )</span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> choice <span class="op">==</span> <span class="st">'rates'</span> :</span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> [<span class="fl">0.0001</span>, <span class="fl">0.001</span>, <span class="fl">0.01</span>, <span class="fl">0.1</span>, <span class="fl">0.2</span>, <span class="fl">0.3</span>,<span class="fl">1.0</span>]:</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>            key <span class="op">=</span> <span class="st">'</span><span class="sc">%.4f</span><span class="st">'</span> <span class="op">%</span> i</span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a>            models[key] <span class="op">=</span> GradientBoostingRegressor(learning_rate<span class="op">=</span>i, n_estimators<span class="op">=</span> n_best, max_depth <span class="op">=</span> depth_best )</span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> choice <span class="op">==</span> <span class="st">'depth'</span> :</span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,<span class="dv">11</span>):</span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a>            models[<span class="bu">str</span>(i)] <span class="op">=</span> GradientBoostingRegressor(max_depth<span class="op">=</span>i,learning_rate <span class="op">=</span> rate_best,n_estimators<span class="op">=</span> n_best)</span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> models</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> [<span class="st">'depth'</span>,<span class="st">'rates'</span>,<span class="st">'estimators'</span>]:</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>    models <span class="op">=</span> get_models_GB(i)</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>    results, names <span class="op">=</span> <span class="bu">list</span>(), <span class="bu">list</span>()</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> name, model <span class="kw">in</span> models.items():</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>        scores <span class="op">=</span> evaluate_model(model, X, y)</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>        results.append(scores)</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>        names.append(name)</span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>    plt.boxplot(results, labels<span class="op">=</span>names, showmeans<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="ss">f'Mean Squared Error depending on the value of the </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="Hyperparameter_selection_files/figure-html/cell-21-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="Hyperparameter_selection_files/figure-html/cell-21-output-2.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="Hyperparameter_selection_files/figure-html/cell-21-output-3.png" class="img-fluid"></p>
</div>
</div>
<p>As we did in the Random Forest part, to see the impact of every hyper parameter, we fix others and play with one.<br>
We can see that increase the value of the depth isn’t necessary since, after 5, MSE is increasing too.<br>
Also, we can conclude the same way with the learning rate, after 0.1, the model isn’t performing better. As when we used the Random Forest, we can see that at a time, performances are becoming more stable, and, even if we increase the number of estimator, MSE isn’t necessary lower.</p>
</section>
</section>
</section>
<section id="comparison-of-models" class="level2">
<h2 class="anchored" data-anchor-id="comparison-of-models">Comparison of models</h2>
<p>We could select a good set of hyperparameters for each previous ensemble method. We will now compare them. For this, we will train the models with the best hyperparameters found on the whole training set, and compare their mean squared errors on the leftover <em>compare set</em>:</p>
<div class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">"figure.figsize"</span>] <span class="op">=</span> (<span class="dv">8</span>,<span class="dv">4</span>)</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>names <span class="op">=</span> [<span class="st">"Training set"</span>, <span class="st">"Test set"</span>]</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>model_mses <span class="op">=</span> {<span class="st">"Bagging"</span>: [], </span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>              <span class="st">"Random forest"</span>: [], </span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>              <span class="st">"AdaBoost"</span>: [],</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>              <span class="st">"Gradient boosting"</span>: []</span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>             }</span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluating the selected models</span></span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a>model_bagging <span class="op">=</span> BaggingRegressor(bootstrap<span class="op">=</span><span class="va">True</span>, bootstrap_features<span class="op">=</span><span class="va">False</span>, <span class="op">**</span>bag_params) </span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a>model_bagging.fit(X0, y0)</span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a>y0_pred <span class="op">=</span> model_bagging.predict(X0)</span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> model_bagging.predict(X_compare)</span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a>model_mses[<span class="st">"Bagging"</span>].append(mean_squared_error(y0_pred, y0))</span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a>model_mses[<span class="st">"Bagging"</span>].append(mean_squared_error(y_pred, y_compare))</span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-18"><a href="#cb29-18" aria-hidden="true" tabindex="-1"></a>model_rf <span class="op">=</span> RandomForestRegressor(<span class="op">**</span>rf_params)</span>
<span id="cb29-19"><a href="#cb29-19" aria-hidden="true" tabindex="-1"></a>model_rf.fit(X0, y0)</span>
<span id="cb29-20"><a href="#cb29-20" aria-hidden="true" tabindex="-1"></a>y0_pred <span class="op">=</span> model_rf.predict(X0)</span>
<span id="cb29-21"><a href="#cb29-21" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> model_rf.predict(X_compare)</span>
<span id="cb29-22"><a href="#cb29-22" aria-hidden="true" tabindex="-1"></a>model_mses[<span class="st">"Random forest"</span>].append(mean_squared_error(y0_pred, y0))</span>
<span id="cb29-23"><a href="#cb29-23" aria-hidden="true" tabindex="-1"></a>model_mses[<span class="st">"Random forest"</span>].append(mean_squared_error(y_pred, y_compare))</span>
<span id="cb29-24"><a href="#cb29-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-25"><a href="#cb29-25" aria-hidden="true" tabindex="-1"></a>model_ada <span class="op">=</span> AdaBoostRegressor(<span class="op">**</span>ada_params)</span>
<span id="cb29-26"><a href="#cb29-26" aria-hidden="true" tabindex="-1"></a>model_ada.fit(X0, y0)</span>
<span id="cb29-27"><a href="#cb29-27" aria-hidden="true" tabindex="-1"></a>y0_pred <span class="op">=</span> model_ada.predict(X0)</span>
<span id="cb29-28"><a href="#cb29-28" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> model_ada.predict(X_compare)</span>
<span id="cb29-29"><a href="#cb29-29" aria-hidden="true" tabindex="-1"></a>model_mses[<span class="st">"AdaBoost"</span>].append(mean_squared_error(y0_pred, y0))</span>
<span id="cb29-30"><a href="#cb29-30" aria-hidden="true" tabindex="-1"></a>model_mses[<span class="st">"AdaBoost"</span>].append(mean_squared_error(y_pred, y_compare))</span>
<span id="cb29-31"><a href="#cb29-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-32"><a href="#cb29-32" aria-hidden="true" tabindex="-1"></a>model_GB <span class="op">=</span> GradientBoostingRegressor(<span class="op">**</span>GB_params.best_params_)</span>
<span id="cb29-33"><a href="#cb29-33" aria-hidden="true" tabindex="-1"></a>model_GB.fit(X0, y0)</span>
<span id="cb29-34"><a href="#cb29-34" aria-hidden="true" tabindex="-1"></a>y0_pred <span class="op">=</span> model_GB.predict(X0)</span>
<span id="cb29-35"><a href="#cb29-35" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> model_GB.predict(X_compare)</span>
<span id="cb29-36"><a href="#cb29-36" aria-hidden="true" tabindex="-1"></a>model_mses[<span class="st">"Gradient boosting"</span>].append(mean_squared_error(y0_pred, y0))</span>
<span id="cb29-37"><a href="#cb29-37" aria-hidden="true" tabindex="-1"></a>model_mses[<span class="st">"Gradient boosting"</span>].append(mean_squared_error(y_pred, y_compare))</span>
<span id="cb29-38"><a href="#cb29-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-39"><a href="#cb29-39" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.arange(<span class="bu">len</span>(names))</span>
<span id="cb29-40"><a href="#cb29-40" aria-hidden="true" tabindex="-1"></a>width <span class="op">=</span> <span class="fl">0.15</span></span>
<span id="cb29-41"><a href="#cb29-41" aria-hidden="true" tabindex="-1"></a>multiplier <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb29-42"><a href="#cb29-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-43"><a href="#cb29-43" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(layout<span class="op">=</span><span class="st">'constrained'</span>)</span>
<span id="cb29-44"><a href="#cb29-44" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> attribute, measurement <span class="kw">in</span> model_mses.items():</span>
<span id="cb29-45"><a href="#cb29-45" aria-hidden="true" tabindex="-1"></a>    offset <span class="op">=</span> width <span class="op">*</span> multiplier</span>
<span id="cb29-46"><a href="#cb29-46" aria-hidden="true" tabindex="-1"></a>    rects <span class="op">=</span> ax.bar(x <span class="op">+</span> offset, measurement, width, label<span class="op">=</span>attribute)</span>
<span id="cb29-47"><a href="#cb29-47" aria-hidden="true" tabindex="-1"></a>    ax.bar_label(rects, padding<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb29-48"><a href="#cb29-48" aria-hidden="true" tabindex="-1"></a>    multiplier <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb29-49"><a href="#cb29-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-50"><a href="#cb29-50" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Mean squared error'</span>)</span>
<span id="cb29-51"><a href="#cb29-51" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Performances of selected models'</span>)</span>
<span id="cb29-52"><a href="#cb29-52" aria-hidden="true" tabindex="-1"></a>ax.set_xticks(x <span class="op">+</span> width, names)</span>
<span id="cb29-53"><a href="#cb29-53" aria-hidden="true" tabindex="-1"></a>ax.legend(loc<span class="op">=</span><span class="st">'upper left'</span>, ncol<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb29-54"><a href="#cb29-54" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="Hyperparameter_selection_files/figure-html/cell-22-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>We observe that all methods display signs of overfitting. Among the models, the gradient boosting algorithm shows the most pronounced overfitting. Nevertheless, gradient boosting also outperforms all other algorithms significantly. Despite its overfitting tendencies, its mean squared error on the comparison set is only half that of the next algorithm. Remarkably, it achieves an exceptionally low error on the training dataset. This algorithm appears to excel in learning the training data while also demonstrating resistance to overfitting.</p>
<p>In summary, it’s evident that gradient boosting performs exceptionally well in comparison to the other methods. Additionally, it’s noteworthy that it was the fastest algorithm for training and hyperparameter search by an order of magnitude.</p>
<p>The underperformance of AdaBoost, especially on the training set, appears perplexing. Either we may have overlooked a crucial parameter setting in its configuration, or the algorithm might not be well-suited for the given regression problem for some unexplained reason.</p>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>